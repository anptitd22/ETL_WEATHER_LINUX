{"timestamp":"2025-08-29T03:15:45.489765","level":"info","event":"DAG bundles loaded: dags-folder, example_dags","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-08-29T03:15:45.490485","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/main.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-08-29T03:15:46.001167Z","level":"error","event":"WARNING: Using incubator modules: jdk.incubator.vector","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:47.710499Z","level":"error","event":"25/08/29 03:15:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:47.871303Z","level":"error","event":"Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:47.871632Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:47.871702Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.089833Z","level":"info","event":"Task instance is in running state","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.090076Z","level":"info","event":" Previous state of the Task instance: queued","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.090166Z","level":"info","event":"Current task name:etl_data_weather.extract_transform_load","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.090229Z","level":"info","event":"Dag name:etl_pipeline","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.090285Z","level":"info","event":"extract data from file","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.092152Z","level":"info","event":"spark.hadoop.fs.s3a.connection.part.upload.timeout = 60000","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.092418Z","level":"info","event":"spark.hadoop.fs.s3a.socket.recv.buffer = 65536","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.092477Z","level":"info","event":"spark.hadoop.fs.s3a.impl.disable.cache = true","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.092510Z","level":"info","event":"spark.hadoop.fs.s3a.connection.request.timeout = 60000","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.092541Z","level":"info","event":"spark.driver.port = 46015","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.092575Z","level":"info","event":"spark.app.initial.jar.urls = spark://69db7b3cc20d:46015/jars/hadoop-aws-3.3.6.jar,spark://69db7b3cc20d:46015/jars/aws-java-sdk-bundle-1.12.787.jar,spark://69db7b3cc20d:46015/jars/ojdbc11.jar","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.092620Z","level":"info","event":"spark.hadoop.fs.s3a.path.style.access = true","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.092653Z","level":"info","event":"spark.driver.host = 69db7b3cc20d","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.092684Z","level":"info","event":"spark.rdd.compress = True","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.092716Z","level":"info","event":"spark.hadoop.fs.s3a.connection.idle.time = 60000","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.092747Z","level":"info","event":"spark.executor.extraClassPath = /opt/spark/jars/ojdbc11.jar","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.092780Z","level":"info","event":"spark.hadoop.fs.s3a.connection.establish.timeout = 5000","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.092813Z","level":"info","event":"spark.hadoop.fs.s3a.access.key = admin","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.092845Z","level":"info","event":"spark.hadoop.fs.s3a.endpoint = http://minio:9000/","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.092908Z","level":"info","event":"spark.app.submitTime = 1756437347806","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.092956Z","level":"info","event":"spark.hadoop.fs.s3a.retry.interval = 500","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.092998Z","level":"info","event":"spark.jars = /opt/spark/jars/hadoop-aws-3.3.6.jar,/opt/spark/jars/aws-java-sdk-bundle-1.12.787.jar,/opt/spark/jars/ojdbc11.jar","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.093034Z","level":"info","event":"spark.hadoop.fs.s3a.connection.maximum = 15","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.093066Z","level":"info","event":"spark.hadoop.fs.s3a.retry.limit = 5","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.093098Z","level":"info","event":"spark.hadoop.fs.s3a.vectored.read.max.merged.size = 2097152","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.093129Z","level":"info","event":"spark.hadoop.fs.s3a.impl = org.apache.hadoop.fs.s3a.S3AFileSystem","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.093157Z","level":"info","event":"spark.hadoop.fs.s3a.vectored.read.min.seek.size = 131072","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.093186Z","level":"info","event":"spark.hadoop.fs.s3a.secret.key = admin12345","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.093284Z","level":"info","event":"spark.sql.adaptive.coalescePartitions.enabled = false","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.093323Z","level":"info","event":"spark.hadoop.fs.s3a.socket.send.buffer = 65536","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.093356Z","level":"info","event":"spark.hadoop.fs.s3a.fast.upload = true","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.093386Z","level":"info","event":"spark.executor.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-modules=jdk.incubator.vector --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dio.netty.tryReflectionSetAccessible=true","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.093418Z","level":"info","event":"spark.sql.artifact.isolation.enabled = false","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.093446Z","level":"info","event":"spark.hadoop.fs.s3a.multipart.size = 104857600","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.093475Z","level":"info","event":"spark.master = local[*]","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.093506Z","level":"info","event":"spark.app.id = local-1756437348808","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.093538Z","level":"info","event":"spark.executor.id = driver","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.093567Z","level":"info","event":"spark.driver.extraClassPath = /opt/spark/jars/ojdbc11.jar","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.093597Z","level":"info","event":"spark.app.name = BAITEST","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.093627Z","level":"info","event":"spark.submit.deployMode = client","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.093657Z","level":"info","event":"spark.hadoop.fs.s3a.connection.ssl.enabled = false","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.093694Z","level":"info","event":"spark.serializer.objectStreamReset = 100","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.093727Z","level":"info","event":"spark.ui.showConsoleProgress = true","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.093756Z","level":"info","event":"spark.hadoop.fs.s3a.multipart.purge.age = 86400","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.093785Z","level":"info","event":"spark.app.startTime = 1756437348045","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.093814Z","level":"info","event":"spark.hadoop.fs.s3a.threads.max = 10","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.093843Z","level":"info","event":"spark.hadoop.fs.s3a.aws.credentials.provider = org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.093872Z","level":"info","event":"spark.submit.pyFiles =","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.093914Z","level":"info","event":"spark.driver.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-modules=jdk.incubator.vector --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dio.netty.tryReflectionSetAccessible=true","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.093946Z","level":"info","event":"spark.hadoop.fs.s3a.attempts.maximum = 3","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.093976Z","level":"info","event":"spark.hadoop.fs.s3a.threads.keepalivetime = 60000","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.094005Z","level":"info","event":"spark.repl.local.jars = file:///opt/spark/jars/hadoop-aws-3.3.6.jar,file:///opt/spark/jars/aws-java-sdk-bundle-1.12.787.jar,file:///opt/spark/jars/ojdbc11.jar","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.094038Z","level":"info","event":"spark.sql.adaptive.enabled = false","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.094067Z","level":"info","event":"spark.hadoop.fs.s3a.connection.timeout = 60000","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:52.940772Z","level":"error","event":"25/08/29 03:15:52 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.014475Z","level":"error","event":"25/08/29 03:15:53 WARN VersionInfoUtils: The AWS SDK for Java 1.x entered maintenance mode starting July 31, 2024 and will reach end of support on December 31, 2025. For more information, see https://aws.amazon.com/blogs/developer/the-aws-sdk-for-java-1-x-is-in-maintenance-mode-effective-july-31-2024/","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.014690Z","level":"error","event":"You can print where on the file system the AWS SDK for Java 1.x core runtime is located by setting the AWS_JAVA_V1_PRINT_LOCATION environment variable or aws.java.v1.printLocation system property to 'true'.","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.014730Z","level":"error","event":"This message can be disabled by setting the AWS_JAVA_V1_DISABLE_DEPRECATION_ANNOUNCEMENT environment variable or aws.java.v1.disableDeprecationAnnouncement system property to 'true'.","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.014780Z","level":"error","event":"The AWS SDK for Java 1.x is being used here:","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.014812Z","level":"error","event":"at java.base/java.lang.Thread.getStackTrace(Thread.java:1619)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.014841Z","level":"error","event":"at com.amazonaws.util.VersionInfoUtils.printDeprecationAnnouncement(VersionInfoUtils.java:81)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.014869Z","level":"error","event":"at com.amazonaws.util.VersionInfoUtils.<clinit>(VersionInfoUtils.java:59)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.014906Z","level":"error","event":"at com.amazonaws.ClientConfiguration.<clinit>(ClientConfiguration.java:95)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.014935Z","level":"error","event":"at org.apache.hadoop.fs.s3a.S3AUtils.createAwsConf(S3AUtils.java:1251)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.014963Z","level":"error","event":"at org.apache.hadoop.fs.s3a.DefaultS3ClientFactory.createS3Client(DefaultS3ClientFactory.java:118)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.014991Z","level":"error","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.bindAWSClient(S3AFileSystem.java:982)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.015018Z","level":"error","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:586)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.015064Z","level":"error","event":"at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3615)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.015110Z","level":"error","event":"at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:554)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.015157Z","level":"error","event":"at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.015190Z","level":"error","event":"at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:55)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.015218Z","level":"error","event":"at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:381)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.015246Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.015276Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.015304Z","level":"error","event":"at scala.Option.getOrElse(Option.scala:201)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.015331Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.015358Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.015384Z","level":"error","event":"at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.015411Z","level":"error","event":"at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.015443Z","level":"error","event":"at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.015471Z","level":"error","event":"at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.015497Z","level":"error","event":"at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.015524Z","level":"error","event":"at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.015550Z","level":"error","event":"at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.015577Z","level":"error","event":"at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.015604Z","level":"error","event":"at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.015630Z","level":"error","event":"at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.015659Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.015693Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.015725Z","level":"error","event":"at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.015754Z","level":"error","event":"at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.015781Z","level":"error","event":"at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.015807Z","level":"error","event":"at scala.collection.immutable.List.foldLeft(List.scala:79)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.015838Z","level":"error","event":"at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.015978Z","level":"error","event":"at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.016027Z","level":"error","event":"at scala.collection.immutable.List.foreach(List.scala:334)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.016067Z","level":"error","event":"at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.016095Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.016124Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.016156Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.016184Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.016211Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.016238Z","level":"error","event":"at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.016265Z","level":"error","event":"at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.016292Z","level":"error","event":"at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.016319Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.016345Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.016372Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.016398Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.016425Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.016451Z","level":"error","event":"at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.016477Z","level":"error","event":"at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.016509Z","level":"error","event":"at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.016536Z","level":"error","event":"at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.016562Z","level":"error","event":"at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.016588Z","level":"error","event":"at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.016614Z","level":"error","event":"at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.016640Z","level":"error","event":"at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.016666Z","level":"error","event":"at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.016691Z","level":"error","event":"at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.016720Z","level":"error","event":"at scala.util.Try$.apply(Try.scala:217)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.016747Z","level":"error","event":"at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.016773Z","level":"error","event":"at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.016800Z","level":"error","event":"at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.016826Z","level":"error","event":"at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.016852Z","level":"error","event":"at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.016878Z","level":"error","event":"at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.016914Z","level":"error","event":"at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.016941Z","level":"error","event":"at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.016967Z","level":"error","event":"at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.016993Z","level":"error","event":"at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.017022Z","level":"error","event":"at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.017048Z","level":"error","event":"at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:296)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.017074Z","level":"error","event":"at org.apache.spark.sql.classic.DataFrameReader.json(DataFrameReader.scala:150)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.017101Z","level":"error","event":"at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.017127Z","level":"error","event":"at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.017156Z","level":"error","event":"at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.017182Z","level":"error","event":"at java.base/java.lang.reflect.Method.invoke(Method.java:569)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.017209Z","level":"error","event":"at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.017235Z","level":"error","event":"at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.017261Z","level":"error","event":"at py4j.Gateway.invoke(Gateway.java:282)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.017287Z","level":"error","event":"at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.017314Z","level":"error","event":"at py4j.commands.CallCommand.execute(CallCommand.java:79)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.017362Z","level":"error","event":"at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.017397Z","level":"error","event":"at py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:53.017426Z","level":"error","event":"at java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-08-29T03:15:54.079989Z","level":"info","event":"spark data extracted","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:54.095498Z","level":"info","event":"root","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:54.095806Z","level":"info","event":" |-- list: array (nullable = true)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:54.095882Z","level":"info","event":" |    |-- element: struct (containsNull = true)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:54.095958Z","level":"info","event":" |    |    |-- dt: long (nullable = true)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:54.096012Z","level":"info","event":" |    |    |-- main: struct (nullable = true)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:54.096066Z","level":"info","event":" |    |    |    |-- temp: double (nullable = true)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:54.096120Z","level":"info","event":" |    |    |    |-- pressure: long (nullable = true)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:54.096172Z","level":"info","event":" |    |    |    |-- humidity: long (nullable = true)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:54.096222Z","level":"info","event":" |    |    |-- weather: array (nullable = true)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:54.096274Z","level":"info","event":" |    |    |    |-- element: struct (containsNull = true)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:54.096323Z","level":"info","event":" |    |    |    |    |-- description: string (nullable = true)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:54.096377Z","level":"info","event":"","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:56.103461Z","level":"info","event":"+--------------------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:56.103871Z","level":"info","event":"|                list|","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:56.103984Z","level":"info","event":"+--------------------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:56.104047Z","level":"info","event":"|[{1752192000, {26...|","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:56.104110Z","level":"info","event":"+--------------------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:56.104170Z","level":"info","event":"","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:57.565139Z","level":"info","event":"+----------+-----------+--------+--------+-----------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:57.565415Z","level":"info","event":"|        dt|temperature|pressure|humidity|description|","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:57.565452Z","level":"info","event":"+----------+-----------+--------+--------+-----------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:57.565484Z","level":"info","event":"|1752192000|      26.72|     998|      89|    mưa vừa|","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:57.565517Z","level":"info","event":"|1752202800|      27.27|     999|      81|    mưa nhẹ|","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:57.565547Z","level":"info","event":"|1752213600|      30.07|     999|      67|    mưa vừa|","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:57.565576Z","level":"info","event":"|1752224400|      29.96|     998|      75|    mưa vừa|","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:57.565627Z","level":"info","event":"|1752235200|      26.19|    1000|      91|    mưa vừa|","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:57.565657Z","level":"info","event":"+----------+-----------+--------+--------+-----------+","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:57.565685Z","level":"info","event":"only showing top 5 rows","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:57.565715Z","level":"info","event":"spark data selected","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:57.565746Z","level":"info","event":"transform data","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:57.721472Z","level":"info","event":"load data","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:57.721765Z","level":"info","event":"(DESCRIPTION=(ADDRESS=(PROTOCOL=TCP)(HOST=orcl-21c)(PORT=1521))(CONNECT_DATA=(SERVICE_NAME=TEST_O)))","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:57.808216Z","level":"info","event":"Table `WEATHER` created successfully.","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.199284Z","level":"info","event":"An error occurred while calling o314.save.","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.199546Z","level":"info","event":": java.sql.SQLRecoverableException: Listener refused the connection with the following error:","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.199659Z","level":"info","event":"ORA-12514, TNS:listener does not currently know of service requested in connect descriptor","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.199731Z","level":"info","event":"  (CONNECTION_ID=pOkHtCEPSACUUbX1C/v5Xg==)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.199778Z","level":"info","event":"\tat oracle.jdbc.driver.T4CConnection.handleLogonNetException(T4CConnection.java:902)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.199825Z","level":"info","event":"\tat oracle.jdbc.driver.T4CConnection.logon(T4CConnection.java:707)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.199876Z","level":"info","event":"\tat oracle.jdbc.driver.PhysicalConnection.connect(PhysicalConnection.java:1094)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.199958Z","level":"info","event":"\tat oracle.jdbc.driver.T4CDriverExtension.getConnection(T4CDriverExtension.java:89)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.200036Z","level":"info","event":"\tat oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:732)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.200094Z","level":"info","event":"\tat oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:648)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.200149Z","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:50)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.200199Z","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.200252Z","level":"info","event":"\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:235)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.200306Z","level":"info","event":"\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:231)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.200359Z","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:51)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.200418Z","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.200500Z","level":"info","event":"\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.200552Z","level":"info","event":"\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.200596Z","level":"info","event":"\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.200640Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.200679Z","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.200732Z","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.200788Z","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.200839Z","level":"info","event":"\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.200909Z","level":"info","event":"\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.200957Z","level":"info","event":"\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.201002Z","level":"info","event":"\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.201054Z","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.201108Z","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.201154Z","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.201206Z","level":"info","event":"\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.201261Z","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.201310Z","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.201354Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.201402Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.201459Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.201530Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.201592Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.201645Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.201695Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.201750Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.201879Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.201995Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.202055Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.202111Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.202165Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.202277Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.202332Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.202390Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.202448Z","level":"info","event":"\tat scala.util.Try$.apply(Try.scala:217)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.202501Z","level":"info","event":"\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.202549Z","level":"info","event":"\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.202605Z","level":"info","event":"\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.202662Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.202724Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.202793Z","level":"info","event":"\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.202860Z","level":"info","event":"\tat org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.203006Z","level":"info","event":"\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:241)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.203075Z","level":"info","event":"\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:126)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.203145Z","level":"info","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.203252Z","level":"info","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.203340Z","level":"info","event":"\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.203434Z","level":"info","event":"\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.203513Z","level":"info","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.203571Z","level":"info","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.203623Z","level":"info","event":"\tat py4j.Gateway.invoke(Gateway.java:282)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.203675Z","level":"info","event":"\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.203738Z","level":"info","event":"\tat py4j.commands.CallCommand.execute(CallCommand.java:79)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.203798Z","level":"info","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.203858Z","level":"info","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.203934Z","level":"info","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.203971Z","level":"info","event":"\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.204027Z","level":"info","event":"\t\tat oracle.jdbc.driver.T4CConnection.handleLogonNetException(T4CConnection.java:902)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.204090Z","level":"info","event":"\t\tat oracle.jdbc.driver.T4CConnection.logon(T4CConnection.java:707)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.204144Z","level":"info","event":"\t\tat oracle.jdbc.driver.PhysicalConnection.connect(PhysicalConnection.java:1094)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.204204Z","level":"info","event":"\t\tat oracle.jdbc.driver.T4CDriverExtension.getConnection(T4CDriverExtension.java:89)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.204261Z","level":"info","event":"\t\tat oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:732)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.204325Z","level":"info","event":"\t\tat oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:648)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.204386Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:50)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.204470Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.204541Z","level":"info","event":"\t\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:235)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.204604Z","level":"info","event":"\t\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:231)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.204663Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:51)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.204730Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.204815Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.204874Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.204970Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.205112Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.205182Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.205268Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.205324Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.205380Z","level":"info","event":"\t\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.205466Z","level":"info","event":"\t\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.205523Z","level":"info","event":"\t\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.205589Z","level":"info","event":"\t\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.205646Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.205699Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.205759Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.205840Z","level":"info","event":"\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.205947Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.206014Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.206068Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.206124Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.206184Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.206342Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.206400Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.206447Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.206491Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.206538Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.206581Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.206648Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.206695Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.206744Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.206797Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.206850Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.206928Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.206985Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.207048Z","level":"info","event":"\t\tat scala.util.Try$.apply(Try.scala:217)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.207182Z","level":"info","event":"\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.207271Z","level":"info","event":"\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.207370Z","level":"info","event":"\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.207436Z","level":"info","event":"\t\t... 19 more","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.207496Z","level":"info","event":"Caused by: oracle.net.ns.NetException: Listener refused the connection with the following error:","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.207549Z","level":"info","event":"ORA-12514, TNS:listener does not currently know of service requested in connect descriptor","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.207621Z","level":"info","event":"  (CONNECTION_ID=pOkHtCEPSACUUbX1C/v5Xg==)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.207683Z","level":"info","event":"\tat oracle.net.ns.NSProtocolNIO.createRefusePacketException(NSProtocolNIO.java:824)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.207759Z","level":"info","event":"\tat oracle.net.ns.NSProtocolNIO.handleConnectPacketResponse(NSProtocolNIO.java:396)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.207827Z","level":"info","event":"\tat oracle.net.ns.NSProtocolNIO.negotiateConnection(NSProtocolNIO.java:207)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.207919Z","level":"info","event":"\tat oracle.net.ns.NSProtocol.connect(NSProtocol.java:354)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.207992Z","level":"info","event":"\tat oracle.jdbc.driver.T4CConnection.connect(T4CConnection.java:2627)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.208064Z","level":"info","event":"\tat oracle.jdbc.driver.T4CConnection.logon(T4CConnection.java:666)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.208289Z","level":"info","event":"\tat oracle.jdbc.driver.PhysicalConnection.connect(PhysicalConnection.java:1094)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.208380Z","level":"info","event":"\tat oracle.jdbc.driver.T4CDriverExtension.getConnection(T4CDriverExtension.java:89)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.208432Z","level":"info","event":"\tat oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:732)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.208478Z","level":"info","event":"\tat oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:648)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.208531Z","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:50)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.208603Z","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.208664Z","level":"info","event":"\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:235)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.208720Z","level":"info","event":"\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:231)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.208778Z","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:51)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.208815Z","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.208866Z","level":"info","event":"\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.208929Z","level":"info","event":"\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.208963Z","level":"info","event":"\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.208991Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.209020Z","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.209048Z","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.209076Z","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.209105Z","level":"info","event":"\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.209134Z","level":"info","event":"\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.209165Z","level":"info","event":"\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.209195Z","level":"info","event":"\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.209221Z","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.209249Z","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.209278Z","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.209306Z","level":"info","event":"\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.209334Z","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.209360Z","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.209387Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.209415Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.209442Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.209477Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.209506Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.209535Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.209568Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.209596Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.209625Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.209656Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.209685Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.209714Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.209742Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.209833Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.209875Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.209930Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.209983Z","level":"info","event":"\tat scala.util.Try$.apply(Try.scala:217)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.210035Z","level":"info","event":"\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.210077Z","level":"info","event":"\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.210108Z","level":"info","event":"\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.210138Z","level":"info","event":"\t... 19 more","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.210169Z","level":"info","event":"","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.538872","level":"error","event":"Task failed with exception","logger":"task","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o314.save.\n: java.sql.SQLRecoverableException: Listener refused the connection with the following error:\nORA-12514, TNS:listener does not currently know of service requested in connect descriptor\n  (CONNECTION_ID=pOkHtCEPSACUUbX1C/v5Xg==)\n\tat oracle.jdbc.driver.T4CConnection.handleLogonNetException(T4CConnection.java:902)\n\tat oracle.jdbc.driver.T4CConnection.logon(T4CConnection.java:707)\n\tat oracle.jdbc.driver.PhysicalConnection.connect(PhysicalConnection.java:1094)\n\tat oracle.jdbc.driver.T4CDriverExtension.getConnection(T4CDriverExtension.java:89)\n\tat oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:732)\n\tat oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:648)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:50)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:235)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:231)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:51)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)\n\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:241)\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:126)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat oracle.jdbc.driver.T4CConnection.handleLogonNetException(T4CConnection.java:902)\n\t\tat oracle.jdbc.driver.T4CConnection.logon(T4CConnection.java:707)\n\t\tat oracle.jdbc.driver.PhysicalConnection.connect(PhysicalConnection.java:1094)\n\t\tat oracle.jdbc.driver.T4CDriverExtension.getConnection(T4CDriverExtension.java:89)\n\t\tat oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:732)\n\t\tat oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:648)\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:50)\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n\t\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:235)\n\t\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:231)\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:51)\n\t\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)\n\t\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)\n\t\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)\n\t\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\n\t\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\t\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)\n\t\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)\n\t\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\n\t\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)\n\t\tat scala.util.Try$.apply(Try.scala:217)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 19 more\nCaused by: oracle.net.ns.NetException: Listener refused the connection with the following error:\nORA-12514, TNS:listener does not currently know of service requested in connect descriptor\n  (CONNECTION_ID=pOkHtCEPSACUUbX1C/v5Xg==)\n\tat oracle.net.ns.NSProtocolNIO.createRefusePacketException(NSProtocolNIO.java:824)\n\tat oracle.net.ns.NSProtocolNIO.handleConnectPacketResponse(NSProtocolNIO.java:396)\n\tat oracle.net.ns.NSProtocolNIO.negotiateConnection(NSProtocolNIO.java:207)\n\tat oracle.net.ns.NSProtocol.connect(NSProtocol.java:354)\n\tat oracle.jdbc.driver.T4CConnection.connect(T4CConnection.java:2627)\n\tat oracle.jdbc.driver.T4CConnection.logon(T4CConnection.java:666)\n\tat oracle.jdbc.driver.PhysicalConnection.connect(PhysicalConnection.java:1094)\n\tat oracle.jdbc.driver.T4CDriverExtension.getConnection(T4CDriverExtension.java:89)\n\tat oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:732)\n\tat oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:648)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:50)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:235)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:231)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:51)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t... 19 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":825,"name":"run"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1088,"name":"_execute_task"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":408,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":212,"name":"execute"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":235,"name":"execute_callable"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/callback_runner.py","lineno":81,"name":"run"},{"filename":"/opt/airflow/dags/main.py","lineno":24,"name":"run_etl_weather"},{"filename":"/opt/airflow/dags/etl_data_weather.py","lineno":86,"name":"load_data_weather"},{"filename":"/opt/airflow/dags/spark/build_spark.py","lineno":87,"name":"write_spark"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":1743,"name":"save"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1362,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":282,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":327,"name":"get_return_value"}]}]}
{"timestamp":"2025-08-29T03:15:58.598149Z","level":"info","event":"Task instance in failure state","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.598255Z","level":"info","event":"Task start","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.598314Z","level":"info","event":"Task:<Task(PythonOperator): etl_data_weather.extract_transform_load>","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.623226Z","level":"info","event":"Failure caused by An error occurred while calling o314.save.","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.623460Z","level":"info","event":": java.sql.SQLRecoverableException: Listener refused the connection with the following error:","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.623520Z","level":"info","event":"ORA-12514, TNS:listener does not currently know of service requested in connect descriptor","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.623556Z","level":"info","event":"  (CONNECTION_ID=pOkHtCEPSACUUbX1C/v5Xg==)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.623585Z","level":"info","event":"\tat oracle.jdbc.driver.T4CConnection.handleLogonNetException(T4CConnection.java:902)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.623614Z","level":"info","event":"\tat oracle.jdbc.driver.T4CConnection.logon(T4CConnection.java:707)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.623641Z","level":"info","event":"\tat oracle.jdbc.driver.PhysicalConnection.connect(PhysicalConnection.java:1094)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.623669Z","level":"info","event":"\tat oracle.jdbc.driver.T4CDriverExtension.getConnection(T4CDriverExtension.java:89)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.623706Z","level":"info","event":"\tat oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:732)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.623735Z","level":"info","event":"\tat oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:648)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.623763Z","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:50)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.623791Z","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.623818Z","level":"info","event":"\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:235)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.623849Z","level":"info","event":"\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:231)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.623901Z","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:51)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.623947Z","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.623989Z","level":"info","event":"\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.624033Z","level":"info","event":"\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.624078Z","level":"info","event":"\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.624124Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.624165Z","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.624208Z","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.624257Z","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.624305Z","level":"info","event":"\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.624354Z","level":"info","event":"\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.624411Z","level":"info","event":"\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.624454Z","level":"info","event":"\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.624496Z","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.624548Z","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.624592Z","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.624636Z","level":"info","event":"\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.624680Z","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.624722Z","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.624769Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.624814Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.624866Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.624925Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.624976Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.625025Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.625075Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.625123Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.625237Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.625295Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.625347Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.625399Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.625451Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.625503Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.625558Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.625624Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.625682Z","level":"info","event":"\tat scala.util.Try$.apply(Try.scala:217)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.625735Z","level":"info","event":"\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.625786Z","level":"info","event":"\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.625839Z","level":"info","event":"\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.625916Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.625971Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.626019Z","level":"info","event":"\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.626070Z","level":"info","event":"\tat org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.626115Z","level":"info","event":"\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:241)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.626161Z","level":"info","event":"\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:126)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.626211Z","level":"info","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.626264Z","level":"info","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.626321Z","level":"info","event":"\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.626379Z","level":"info","event":"\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.626438Z","level":"info","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.626496Z","level":"info","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.626556Z","level":"info","event":"\tat py4j.Gateway.invoke(Gateway.java:282)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.626611Z","level":"info","event":"\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.626667Z","level":"info","event":"\tat py4j.commands.CallCommand.execute(CallCommand.java:79)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.626723Z","level":"info","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.626781Z","level":"info","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.626843Z","level":"info","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.626915Z","level":"info","event":"\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.626985Z","level":"info","event":"\t\tat oracle.jdbc.driver.T4CConnection.handleLogonNetException(T4CConnection.java:902)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.627046Z","level":"info","event":"\t\tat oracle.jdbc.driver.T4CConnection.logon(T4CConnection.java:707)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.627102Z","level":"info","event":"\t\tat oracle.jdbc.driver.PhysicalConnection.connect(PhysicalConnection.java:1094)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.627163Z","level":"info","event":"\t\tat oracle.jdbc.driver.T4CDriverExtension.getConnection(T4CDriverExtension.java:89)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.627214Z","level":"info","event":"\t\tat oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:732)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.627265Z","level":"info","event":"\t\tat oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:648)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.627318Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:50)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.627404Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.627465Z","level":"info","event":"\t\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:235)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.627517Z","level":"info","event":"\t\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:231)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.627560Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:51)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.627593Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.627625Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.627655Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.627682Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.627781Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.627825Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.627854Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.627882Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.627931Z","level":"info","event":"\t\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.627961Z","level":"info","event":"\t\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.627990Z","level":"info","event":"\t\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.628018Z","level":"info","event":"\t\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.628046Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.628074Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.628101Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.628128Z","level":"info","event":"\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.628155Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.628182Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.628208Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.628235Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.628261Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.628288Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.628314Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.628340Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.628367Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.628394Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.628421Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.628452Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.628480Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.628507Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.628535Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.628563Z","level":"info","event":"\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.628590Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.628616Z","level":"info","event":"\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.628644Z","level":"info","event":"\t\tat scala.util.Try$.apply(Try.scala:217)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.628670Z","level":"info","event":"\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.628697Z","level":"info","event":"\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.628724Z","level":"info","event":"\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.628752Z","level":"info","event":"\t\t... 19 more","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.628781Z","level":"info","event":"Caused by: oracle.net.ns.NetException: Listener refused the connection with the following error:","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.628812Z","level":"info","event":"ORA-12514, TNS:listener does not currently know of service requested in connect descriptor","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.628842Z","level":"info","event":"  (CONNECTION_ID=pOkHtCEPSACUUbX1C/v5Xg==)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.628870Z","level":"info","event":"\tat oracle.net.ns.NSProtocolNIO.createRefusePacketException(NSProtocolNIO.java:824)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.628913Z","level":"info","event":"\tat oracle.net.ns.NSProtocolNIO.handleConnectPacketResponse(NSProtocolNIO.java:396)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.628942Z","level":"info","event":"\tat oracle.net.ns.NSProtocolNIO.negotiateConnection(NSProtocolNIO.java:207)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.628971Z","level":"info","event":"\tat oracle.net.ns.NSProtocol.connect(NSProtocol.java:354)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.628999Z","level":"info","event":"\tat oracle.jdbc.driver.T4CConnection.connect(T4CConnection.java:2627)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.629028Z","level":"info","event":"\tat oracle.jdbc.driver.T4CConnection.logon(T4CConnection.java:666)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.629085Z","level":"info","event":"\tat oracle.jdbc.driver.PhysicalConnection.connect(PhysicalConnection.java:1094)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.629120Z","level":"info","event":"\tat oracle.jdbc.driver.T4CDriverExtension.getConnection(T4CDriverExtension.java:89)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.629158Z","level":"info","event":"\tat oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:732)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.629188Z","level":"info","event":"\tat oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:648)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.629216Z","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:50)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.629245Z","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.629275Z","level":"info","event":"\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:235)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.629302Z","level":"info","event":"\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:231)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.629330Z","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:51)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.629358Z","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.629385Z","level":"info","event":"\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.629413Z","level":"info","event":"\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.629442Z","level":"info","event":"\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.629470Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.629497Z","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.629526Z","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.629553Z","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.629580Z","level":"info","event":"\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.629607Z","level":"info","event":"\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.629635Z","level":"info","event":"\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.629662Z","level":"info","event":"\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.629691Z","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.629734Z","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.629769Z","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.629802Z","level":"info","event":"\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.629832Z","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.629861Z","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.629912Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.629944Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.629990Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.630035Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.630083Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.630128Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.630181Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.630235Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.630280Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.630328Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.630387Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.630439Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.630498Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.630643Z","level":"info","event":"\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.630716Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.630777Z","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.630827Z","level":"info","event":"\tat scala.util.Try$.apply(Try.scala:217)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.630874Z","level":"info","event":"\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.630946Z","level":"info","event":"\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.630995Z","level":"info","event":"\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.631047Z","level":"info","event":"\t... 19 more","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:58.631094Z","level":"info","event":"","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-29T03:15:59.023616Z","level":"error","event":"\r[Stage 0:===========================================================(1 + 0) / 1]","chan":"stderr","logger":"task"}
